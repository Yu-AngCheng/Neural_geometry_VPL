
A neural geometry theory comprehensively explains apparently conflicting models of visual perceptual learning.<br>

Overview
========
Visual perceptual learning (VPL), defined as long-term improvement in a visual task, is considered a crucial tool for elucidating underlying visual and brain plasticity. However, the identification of a unified theory of VPL has long been controversial. Multiple existing models have proposed diverse mechanisms, including improved signal-to-noise ratio, changes in tuning curves, and reduction of noise correlations, as major contributors to improved neural representations associated with VPL. However, each model only accounts for specific aspects of the empirical findings, and there exists no theory that can comprehensively explain all empirical results. Here, we argue that all neural changes at single units can be conceptualized as geometric transformations of population response manifolds in a high-dimensional neural space. This approach enables conflicting major models of VPL to be quantitatively tested and compared within a unified computational theory. Following this approach, we found that changes in tuning curves and noise correlations, as emphasized by previous models, make no significant contributions to improved population representations by visual training. Instead, we identified neural manifold shrinkage due to reduced trial-by-trial neural response variability, a previously unexplored factor, as the primary mechanism underlying improved population representations. Furthermore, we showed that manifold shrinkage successfully accounts for learning effects across various domains, including artificial neural responses in deep neural networks trained on typical VPL tasks, multivariate BOLD signals in humans, and multi-unit activities in monkeys. These converging results suggest that our neural geometry theory offers a quantitative and comprehensive approach to explain a wide range of empirical results and to reconcile previously conflicting models of VPL.

# Citation
This repository was released with the following pre-print. If you use this repository in your research, please cite as:

[Cheng, Y. A., Sanayei, M., Chen, X., Jia, K., Li, S., Fang, F., ... & Zhang, R. Y. (2023). A neural geometry theory comprehensively explains apparently conflicting models of visual perceptual learning. bioRxiv, 2023-11.]

```
@article {Cheng2023.11.13.566963,
	author = {Cheng, Yu-Ang and Sanayei, Mehdi and Chen, Xing and Jia, Ke and Li, Sheng and Fang, Fang and Watanabe, Takeo and Thiele, Alexander and Zhang, Ru-Yuan},
	title = {A neural geometry theory comprehensively explains apparently conflicting models of visual perceptual learning},
	elocation-id = {2023.11.13.566963},
	year = {2024},
	doi = {10.1101/2023.11.13.566963},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Visual perceptual learning (VPL), defined as long-term improvement in a visual task, is considered a crucial tool for elucidating underlying visual and brain plasticity. However, the identification of a unified theory of VPL has long been controversial. Multiple existing models have proposed diverse mechanisms, including improved signal-to-noise ratio, changes in tuning curves, and reduction of noise correlations, as major contributors to improved neural representations associated with VPL. However, each model only accounts for specific aspects of the empirical findings, and there exists no theory that can comprehensively explain all empirical results. Here, we argue that all neural changes at single units can be conceptualized as geometric transformations of population response manifolds in a high-dimensional neural space. This approach enables conflicting major models of VPL to be quantitatively tested and compared within a unified computational theory. Following this approach, we found that changes in tuning curves and noise correlations, as emphasized by previous models, make no significant contributions to improved population representations by visual training. Instead, we identified neural manifold shrinkage due to reduced trial-by-trial neural response variability, a previously unexplored factor, as the primary mechanism underlying improved population representations. Furthermore, we showed that manifold shrinkage successfully accounts for learning effects across various domains, including artificial neural responses in deep neural networks trained on typical VPL tasks, multivariate BOLD signals in humans, and multi-unit activities in monkeys. These converging results suggest that our neural geometry theory offers a quantitative and comprehensive approach to explain a wide range of empirical results and to reconcile previously conflicting models of VPL.Competing Interest StatementDr. Xing Chen a cofounder and shareholder of a neurotechnology start-up, Phosphoenix (Netherlands). Other authors declare no competing financial interests.},
	URL = {https://www.biorxiv.org/content/early/2024/05/19/2023.11.13.566963},
	eprint = {https://www.biorxiv.org/content/early/2024/05/19/2023.11.13.566963.full.pdf},
	journal = {bioRxiv}
}
```

# Authors
* **Yu-Ang Cheng** ([https://github.com/Yu-AngCheng](https://github.com/Yu-AngCheng))

# License
This project is licensed under the MIT License
